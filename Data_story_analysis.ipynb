{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579d5f83-5f2b-4c66-88d9-b431f81fc88e",
   "metadata": {},
   "source": [
    "**This is the final notebook for Milestone 3, where all the data analysis (besides the pre processing done in Milestone 2) is done.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9869f0-3c97-41a8-9607-7e687124bec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\tsfei\\anaconda3\\envs\\ada\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import bz2\n",
    "import datetime\n",
    "import sys\n",
    "from empath import Empath\n",
    "import json\n",
    "import glob\n",
    "import copy\n",
    "#from src.prep_utilities import * \n",
    "#from src.prep_pipeline import *\n",
    "\n",
    "# Load nltk models\n",
    "#!python ./src/load_models_data.py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!python ./src/load_models_data.py\n",
    "data_folder = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4b783-5da9-4b16-9509-02f235c96892",
   "metadata": {},
   "source": [
    "# General Gender Bias Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303118b-78e0-4843-a49d-a32d742ee8c5",
   "metadata": {},
   "source": [
    "In this section, we plan to analyse:\n",
    " - Evolution of percentage of speakers by gender, over time\n",
    " - Evolution of percentage of quotations by gender, over time\n",
    " - Most quoted speakers by gender, over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfe526-ce0b-46a1-a259-81af57e00d63",
   "metadata": {},
   "source": [
    "We'll do a month-by-month analysis. For each month/gender combination, we'll save a Dataframe with the speakers and their total number of quotations that month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb66d116-43d5-48a7-9b61-18e0caa774b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time ranges\n",
    "years = range(2015,2021)\n",
    "min_month = \"2015-01\"\n",
    "max_month = \"2020-04\"\n",
    "all_months = pd.period_range(min_month, max_month, freq='M')\n",
    "\n",
    "genders = ['male', 'female', 'transgender male', 'transgender female', 'non-binary', 'genderfluid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595d55ad-2d98-4273-846c-7ed0f62656ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionaries we'll use to store analysis\n",
    "\n",
    "speaker_df = {} # Save the dataframes of top speakers by gender/month\n",
    "\n",
    "for gender in genders:  \n",
    "    speakers_df_temp = {}\n",
    "    \n",
    "    for month in all_months:\n",
    "\n",
    "        speakers_df_temp[month] = pd.DataFrame([], columns = ['speaker', 'numOccurrences']).set_index('speaker')\n",
    "\n",
    "    speaker_df[gender] = speakers_df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9410e3e-bef4-49b8-a2d9-efe7de556318",
   "metadata": {},
   "source": [
    "The pre-processed data will be loaded each year, by chunks. In the analyses, we will cycle through each year, and through each chunk, and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318cc39e-f9f2-4227-a606-b6d1821e93b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing year 2015... Done in 1399.95s\n",
      "Analysing year 2016... Done in 829.45s\n",
      "Analysing year 2017... Done in 2002.01s\n",
      "Analysing year 2018... Done in 2067.94s\n",
      "Analysing year 2019... Done in 1489.73s\n",
      "Analysing year 2020... Done in 164.27s\n"
     ]
    }
   ],
   "source": [
    "# Loop through years\n",
    "for year in years:\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    # data location and chunk size\n",
    "    data_file = 'quotes-'+ str(year)+'-prep.json.bz2'\n",
    "    data_path = data_folder + data_file\n",
    "    chunk_size = 1e4\n",
    "\n",
    "    # Load by chunks\n",
    "    f = bz2.open(data_path, \"rb\")\n",
    "    data=pd.read_json(f, lines=True, chunksize=chunk_size)\n",
    "    \n",
    "    print(f\"Analysing year {year}...\", end=\" \")\n",
    "    \n",
    "\n",
    "    # Loop through chunks\n",
    "    for i_chunk, chunk in enumerate(data):\n",
    "        \n",
    "        ## Run analysis ##\n",
    "        \n",
    "        # Create range of months for this year\n",
    "        if year != 2020:\n",
    "            months = pd.period_range(str(year)+'-'+'01', str(year)+'-'+'12', freq='M')\n",
    "        else:\n",
    "            months = pd.period_range(str(year)+'-'+'01', str(year)+'-'+'04', freq='M')\n",
    "\n",
    "        # Loop through months\n",
    "        for month in months:\n",
    "            # Mask to select desired month\n",
    "            month_mask = (chunk['date'].dt.to_period('m') == month)\n",
    "            \n",
    "            # Loop through genders\n",
    "            for gender in genders:\n",
    "                \n",
    "                # Mask to select desired gender\n",
    "                gender_mask = (chunk['gender'] == gender)\n",
    "                \n",
    "                # Concatenate the speakers in this chunk with our dictionary\n",
    "                df = chunk[gender_mask & month_mask].groupby(\"speaker\").sum()\n",
    "                speaker_df[gender][month] = pd.concat([speaker_df[gender][month],df]).groupby(\"speaker\").sum()\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print(f\"Done in {stop-start:.2f}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a8e76-f638-4cd4-8cf6-004d20d50a2d",
   "metadata": {},
   "source": [
    "Since we don't want to run the previous cell everytime we reload the notebook, we'll save each of the gender/month combinations to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96efe899-da28-4dee-8879-76aa0d5406ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_analysis_folder = './data_processed/'\n",
    "\n",
    "for gender in genders:\n",
    "    for month in all_months:\n",
    "        with bz2.open(general_analysis_folder + gender +'-' + str(month.year) + '-' + str(month.month) + '.json.bz2', \"w\") as f:\n",
    "\n",
    "            # Write to file, reset index to keep the speaker's names\n",
    "            write = speaker_df[gender][month].reset_index().to_json(f, lines=True, orient='records') \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167305f-0efc-434f-a566-7016fe3a9b39",
   "metadata": {},
   "source": [
    "Now that we've created the files with the analysis data, let's read them again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c421df4-17b9-46d6-b13d-aa58cc34f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_df = {} # Load the dataframes of top speakers by gender/month\n",
    "general_analysis_folder = './data_processed/'\n",
    "\n",
    "for gender in genders:  \n",
    "    speakers_df_temp = {}\n",
    "    for month in all_months:\n",
    "        speakers_df_temp[month] = pd.read_json(general_analysis_folder + gender +'-' + str(month.year) + '-' + str(month.month) + '.json.bz2', lines = True,compression = 'bz2')\n",
    "        \n",
    "        # Join rows of presidential aliases (President Trump, Donald Trump, etc...)\n",
    "        speakers_df_temp[month] = speakers_df_temp[month].replace([\"President Barack Obama\", \"President Obama\"], \"Barack Obama\")\n",
    "        speakers_df_temp[month] = speakers_df_temp[month].replace([\"President Donald Trump\", \"President Trump\"], \"Donald Trump\")\n",
    "        speakers_df_temp[month] = speakers_df_temp[month].groupby('speaker').sum().reset_index()\n",
    "\n",
    "    speaker_df[gender] = speakers_df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3df3b-023d-423b-8776-92465630fb02",
   "metadata": {},
   "source": [
    "## Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07593b46",
   "metadata": {},
   "source": [
    "### Percentage of quotations and speakers by gender\n",
    "\n",
    "We will use the package `plotly` to make an interactive plot with our data in the cells below.\n",
    "\n",
    "To view the plot, double click `plotly/general_quotations_speakers.html`.\n",
    "\n",
    "First, we create DataFrames to hold the plot data, and then we plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d78614a-4a87-4ad1-9ea8-496f23c81615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data to be plotted ##\n",
    "\n",
    "# x data\n",
    "month_str = [str(month.year)+'-'+str(month.month) for month in all_months]\n",
    "month_dt = [datetime.datetime.strptime(x, '%Y-%m') for x in month_str]\n",
    "\n",
    "# y data\n",
    "total_quotations = {} # total quotations by month\n",
    "total_speakers = {} # total number of speakers by month\n",
    "\n",
    "# Dictionaries with total values for each month\n",
    "for month in all_months:\n",
    "    total_quotations[month] = 0\n",
    "    total_speakers[month] = 0\n",
    "\n",
    "    for gender in genders:\n",
    "        total_quotations[month] += speaker_df[gender][month]['numOccurrences'].sum()\n",
    "        total_speakers[month] += len(speaker_df[gender][month])\n",
    "\n",
    "perc_quotations = pd.DataFrame([], columns = genders) # df with the dates and percentage of quotations by gender\n",
    "perc_speakers =  pd.DataFrame([], columns = genders) # df with the dates and percentage of speakers by gender\n",
    "\n",
    "for i,month in enumerate(all_months):\n",
    "    perc_quotations.loc[i] = [100*speaker_df[gender][month]['numOccurrences'].sum()/total_quotations[month] for gender in genders]\n",
    "    perc_speakers.loc[i] = [100*len(speaker_df[gender][month])/total_speakers[month] for gender in genders]\n",
    "    \n",
    "perc_quotations['date'] = month_dt # Dates for x axis\n",
    "perc_speakers['date'] = month_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c4b11",
   "metadata": {},
   "source": [
    "Run the cell below to generate the \"Percentage of Occurrences by Gender\" plot. Double click `plotly/perc_quotations.html` to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412584b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Color palette\n",
    "palette = px.colors.qualitative.Plotly\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "fig.update_layout(title = 'Percentage of Occurrences by Gender')\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.update_yaxes(title='Percentage of Occurrences')\n",
    "\n",
    "# Add plots (they have to be added by order, in order not to mess up the 'visible' lists)\n",
    "for i,gender in enumerate(genders):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x = perc_quotations['date'], y = perc_quotations[gender], name = gender, mode='lines', line=dict(color=palette[i], width=3))\n",
    "        )\n",
    "\n",
    "\n",
    "visible_quotations = [True if i<6 else False for i in range(12)]\n",
    "visible_speakers = [not x for x in visible_quotations]\n",
    "\n",
    "# Add x range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/perc_quotations.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a084b",
   "metadata": {},
   "source": [
    "Run the cell below to generate the \"Percentage of Speakers by Gender\" plot. Double click `plotly/perc_speakers.html` to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97c12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Color palette\n",
    "palette = px.colors.qualitative.Plotly\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "fig.update_layout(title = 'Percentage of Speakers by Gender')\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.update_yaxes(title='Percentage of Speakers')\n",
    "\n",
    "# Add plots\n",
    "for i,gender in enumerate(genders):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x = perc_speakers['date'], y = perc_speakers[gender], name = gender, mode='lines', line=dict(color=palette[i], width=3))\n",
    "        )\n",
    "\n",
    "# Add x range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"./plotly/perc_speakers.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868cf9e",
   "metadata": {},
   "source": [
    "### Top speakers by gender/month plot\n",
    "\n",
    "Once again, we'll use `plotly` for the interactive plot for the highest quoted speaker by gender, in each month.\n",
    "\n",
    "For this, we'll create a new DataFrame with columns `gender`, `speaker`, `date` and `occurrences` (which is the same as `numOccurrences`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80aaba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers_df = pd.DataFrame([], columns = ['gender', 'speaker', 'date', 'occurrences'])\n",
    "\n",
    "i = 0\n",
    "for gender in genders:\n",
    "    for j,month in enumerate(all_months):\n",
    "        # Get top speaker by gender/month\n",
    "        speaker, occurrences = speaker_df[gender][month].sort_values(by='numOccurrences', ascending = False).iloc[0]\n",
    "        top_speakers_df.loc[i] = [gender, speaker, month_str[j], int(occurrences)]\n",
    "        i+=1\n",
    "    i+=1\n",
    "\n",
    "# Convert numOccurrences column to int\n",
    "top_speakers_df.occurrences = top_speakers_df.occurrences.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "274ea918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2015-1</td>\n",
       "      <td>37099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2015-2</td>\n",
       "      <td>32215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>Jesus `` Chuy '' Garcia</td>\n",
       "      <td>2015-3</td>\n",
       "      <td>33467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2015-4</td>\n",
       "      <td>27154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2015-5</td>\n",
       "      <td>14155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>genderfluid</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>genderfluid</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2020-1</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>genderfluid</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2020-2</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>genderfluid</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2020-3</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>genderfluid</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2020-4</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gender                  speaker     date  occurrences\n",
       "0           male             Barack Obama   2015-1        37099\n",
       "1           male             Barack Obama   2015-2        32215\n",
       "2           male  Jesus `` Chuy '' Garcia   2015-3        33467\n",
       "3           male             Barack Obama   2015-4        27154\n",
       "4           male             Barack Obama   2015-5        14155\n",
       "..           ...                      ...      ...          ...\n",
       "384  genderfluid              Miley Cyrus  2019-12          347\n",
       "385  genderfluid              Miley Cyrus   2020-1          180\n",
       "386  genderfluid              Miley Cyrus   2020-2          132\n",
       "387  genderfluid              Miley Cyrus   2020-3         1147\n",
       "388  genderfluid              Miley Cyrus   2020-4          230\n",
       "\n",
       "[384 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_speakers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f031a",
   "metadata": {},
   "source": [
    "We'll now create a bar plot with the highest quoted speakers of each gender, and add an animation slider to move between months. We save it to `plotly/top_speakers.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74cc57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    top_speakers_df, \n",
    "    x='gender', \n",
    "    y = 'quotations', \n",
    "    animation_frame='date', \n",
    "    text = 'speaker',\n",
    "    hover_name='speaker',\n",
    "    hover_data={'gender':False, 'speaker':False, 'quotations':True, 'date':True},\n",
    "    color='gender',\n",
    "    title='Highest Quoted Speakers'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(range=[0, 400000])\n",
    "\n",
    "fig.update_layout(\n",
    "    uniformtext_minsize=13,\n",
    "    uniformtext_mode='show', \n",
    "    showlegend=False,\n",
    "    hovermode='x',\n",
    "    yaxis=dict( # Disable yaxis\n",
    "        visible = True\n",
    "    ),\n",
    "    xaxis=dict( # Remove xaxis title\n",
    "        title=''\n",
    "    ),\n",
    "    hoverlabel=dict( # Change font on hover tool\n",
    "        font_size=16,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/top_speakers.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7d74d-4c50-46d0-9283-c720aadefc90",
   "metadata": {},
   "source": [
    "# Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38040be8-43dd-4e36-908d-37c7b80e7a1e",
   "metadata": {},
   "source": [
    "Like on the section above, we'll start by retrieving the relevant information from the pre-processed data. The code to do that is presented below.\n",
    "\n",
    "To analyze the data, we'll run it through empath to determine what is the number of words spoken about each category. Before we do that, we'll add two new categories to empath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc845b-0705-47c4-b3a3-816850bf1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our empath analyzer\n",
    "lexicon = Empath()\n",
    "\n",
    "# and add two categories to it\n",
    "lexicon.create_category('climate_change', ['global_warming','green_house','death','water','fossil_fuel','burning','summit','environment','energy','renewable','consumption','petrol','gas','wind','solar_power','earth'], model='nytimes')\n",
    "lexicon.create_category('lgbt', ['rights', 'gay', 'trans', 'discriminantion', 'phobia', 'lesbian', 'transsexual','cis','queer','asexual','heterosexual','straight'], model='nytimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da2ca2-74f7-440a-8d01-80c028fdce22",
   "metadata": {},
   "source": [
    "Now we can process all the data by chuncks, and store the results to use later. The results will be written on one file per year, inside `./data_processed` and they will be called `empath_<year>.txt`.\n",
    "\n",
    "The internal structure of these files is the following.\n",
    "- First line: Distribution of the number of words per gender, per month. This information is displayed inside of a dictionary, where the keys are the genders found for that year, and to each key we have associated a list with 12 entries, one for each month, containing the number of words said by that gender in that month.\n",
    "- Second line: Number of chunks processed. No real functional purpose but it allowed to restart the program from a certain point if it got interrupted.\n",
    "- Third line: All the information extracted from the data. This information is displayed inside of a dictionary, where the keys correspond to the genders found for that year. Each key points to a list with 12 entries, one for each month. And in each entry of the list there is another dictionary, which is the output of empath for that month, where the keys correspond to the topics 'eating', 'alcohol', 'cleaning', 'sports',..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38233d2-c223-4dd3-ad09-8806063b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_quotes = []\n",
    "# read all the pre-processed files and store them\n",
    "for file in glob.glob(data_folder + data_file):\n",
    "    dfs_quotes.append(pd.read_json(file, lines=True, chunksize=1e4))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# create a set with all the stopwords so we can remove them\n",
    "stop_words = set(stopwords.words())\n",
    "# iterate through all the files, each one corresponding to one year\n",
    "for year, file in enumerate(dfs_quotes):\n",
    "    themes = {}\n",
    "    n_words = {}\n",
    "    i = 0\n",
    "    # we need to read the file in chuncks, they are too big\n",
    "    for chunk in file:\n",
    "        tokens = {}\n",
    "        # extract the info about quotes, dates and genders\n",
    "        quotes = chunk['tokens'].tolist()\n",
    "        date = chunk['date'].tolist()\n",
    "        gender = chunk['gender'].tolist()\n",
    "        clean_quotes = []\n",
    "        for index, words in enumerate(quotes):\n",
    "            # remove stopwords and join the split tokens\n",
    "            processed_quote = [word for word in words if word not in stop_words]\n",
    "            clean_quotes.append(' '.join(processed_quote))\n",
    "            # if we have not yet added this gender to the number of words, we add it\n",
    "            if gender[index] not in n_words.keys():\n",
    "                n_words[gender[index]] = [0 for _ in range(12)]\n",
    "            # and then we sum the words in this quote\n",
    "            n_words[gender[index]][int(str(date[index])[5:7]) - 1] += len(processed_quote)\n",
    "\n",
    "        # now we divide all the quotes by their dates and genders, to make it easier to process them\n",
    "        for index, quote in enumerate(clean_quotes):\n",
    "            if gender[index] not in tokens.keys():\n",
    "                tokens[gender[index]] = [[] for _ in range(12)]\n",
    "            tokens[gender[index]][int(str(date[index])[5:7]) - 1].append(quote)\n",
    "        \n",
    "        # and finally we iterate through all the genders and all the months....\n",
    "        for gender in tokens.keys():\n",
    "            # we create the necessary entries in the dict\n",
    "            if gender not in themes.keys():\n",
    "                themes[gender] = [{} for _ in range(12)]\n",
    "                # and we analyze the quotes by topics\n",
    "                for month,quotes in enumerate(tokens[gender]):\n",
    "                    themes[gender][month]= lexicon.analyze(quotes, normalize = False)\n",
    "            # if the gender was already in the dictionary, we add the new info to the info that was already there\n",
    "            else:\n",
    "                for month,quotes in enumerate(tokens[gender]):\n",
    "                    themes_partial = lexicon.analyze(quotes, normalize = False)\n",
    "                    themes[gender][month] = {k: themes[gender][month].get(k, 0) + themes_partial.get(k, 0) for k in themes[gender][month].keys() | themes_partial.keys()}\n",
    "\n",
    "        i += 1\n",
    "        # we write the info to the file every 10 chunks so we don't have to start over if it crashes\n",
    "        if i % 10 == 0:\n",
    "            with open(f'./data_processed/empath_{year + 2015}.txt', 'w') as f:\n",
    "                f.write(f'Num words: {n_words}\\n')\n",
    "                f.write(f'Chunks processed: {i}\\n')\n",
    "                f.write(json.dumps(themes))\n",
    "            print(i, end = ',')\n",
    "\n",
    "    # at the end we write it all one last time\n",
    "    with open(f'./data_processed/empath_{year + 2015}.txt', 'w') as f:\n",
    "        f.write(f'Num words: {n_words} \\n')\n",
    "        f.write(f'Chunks processed: {i} \\n')\n",
    "        f.write(json.dumps(themes))\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f'Time to analyze all chunks {timeit.default_timer() - start}!!')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa808581-7069-4679-8805-f472fec552f2",
   "metadata": {},
   "source": [
    "After the analysis is done, we can simply get the relevant data from the saved files, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10bcb7f9-c2fc-4e22-a427-2b52ba8b1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the fully processed data\n",
    "empath_files = glob.glob('./data_processed/empath*.txt')\n",
    "\n",
    "n_words = []\n",
    "empath_data = []\n",
    "\n",
    "# and save it so we can use it\n",
    "for file in empath_files:\n",
    "    with open(file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        txt = txt.split('\\n')\n",
    "        \n",
    "        n_words.append(eval(txt[0][11:]))\n",
    "        empath_data.append(eval(txt[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1bd61a-2aa2-4aaf-855d-90968ba09554",
   "metadata": {},
   "source": [
    "We also save this data as a normalized version, because it will be useful for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c521d5b4-c1c6-4b7a-8e02-4d29eb4bdb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deep copy to store the normalized version of our data\n",
    "empath_data_norm = copy.deepcopy(empath_data)\n",
    "\n",
    "for year, data in enumerate(empath_data_norm):\n",
    "    for gender in data:\n",
    "        for month, datum in enumerate(data[gender]):\n",
    "            for key in datum:\n",
    "                # divide each entry by the number of words of that year, gender, month\n",
    "                if n_words[year][gender][month] != 0:\n",
    "                    datum[key] = datum.get(key, 1) / n_words[year][gender][month] * 100   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090d9ba-2f45-41fe-8d1b-fd15faa09166",
   "metadata": {},
   "source": [
    "Now we convert these dictionaries into a list of tuples, so that we can sort them, while keeping only the main list of genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77a8601-5072-44a8-9901-e3a58aeb2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = []\n",
    "# define the key genders to consider\n",
    "genders = ['male', 'female', 'transgender male', 'transgender female', 'non-binary', 'genderfluid']\n",
    "\n",
    "# iterate through all our data\n",
    "for data in empath_data:\n",
    "    temp_data = {}\n",
    "    # for all the gender we're considering\n",
    "    for gender in data:\n",
    "        # if we find one of them\n",
    "        if gender in genders:\n",
    "            # then we add the entry to the temporary dict\n",
    "            temp_data[gender] = [[] for _ in range(12)]\n",
    "            # and we convert the empath dict into a list and sort it\n",
    "            for month, datum in enumerate(data[gender]):\n",
    "                temp_data[gender][month] = list(datum.items())\n",
    "                temp_data[gender][month].sort(key=lambda tup:tup[1], reverse=True)\n",
    "    sorted_data.append(temp_data)\n",
    "\n",
    "# this process is identical but for the sorted data\n",
    "sorted_data_norm = []\n",
    "for data in empath_data_norm:\n",
    "    temp_data = {}\n",
    "    for gender in data:\n",
    "        if gender in genders:\n",
    "            temp_data[gender] = [[] for _ in range(12)]\n",
    "            for month, datum in enumerate(data[gender]):\n",
    "                temp_data[gender][month] = list(datum.items())\n",
    "                temp_data[gender][month].sort(key=lambda top:top[1], reverse=True)\n",
    "    sorted_data_norm.append(temp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9956c-0ba5-458f-a235-34a52f8604fd",
   "metadata": {},
   "source": [
    "With them sorted, we can prepare the information to start plotting it. For that, let's only consider a subset of key topics:\n",
    "- Business\n",
    "- Sports\n",
    "- Government\n",
    "- Climate Change\n",
    "- LGBT\n",
    "- Money\n",
    "- Family\n",
    "- Health\n",
    "\n",
    "For only these topics, we'll save their information, both in raw and normalized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7d3c5fb-c30a-4ac6-8827-2753abbc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for our topics\n",
    "topics = {'business': {},\n",
    "          'sports': {},\n",
    "          'government': {},\n",
    "          'climate_change': {},\n",
    "          'lgbt': {},\n",
    "          'money': {},\n",
    "          'family': {},\n",
    "          'health': {}\n",
    "         }\n",
    "\n",
    "# iterate through all our sorted data\n",
    "for year, data in enumerate(sorted_data):\n",
    "    for gender in data:\n",
    "        for datum in data[gender]:\n",
    "            for topic in datum:\n",
    "                # if the topic is one we're looking into then we add the words from eampth\n",
    "                if topic[0] in topics.keys():\n",
    "                    if gender in topics[topic[0]].keys():\n",
    "                        topics[topic[0]][gender].append(topic[1])\n",
    "                    else:\n",
    "                        topics[topic[0]][gender] = [topic[1]]\n",
    "                        \n",
    "# identical process but for the normalized data\n",
    "topics_norm = {'business': {},\n",
    "               'sports': {},\n",
    "               'government': {},\n",
    "               'climate_change': {},\n",
    "               'lgbt': {},\n",
    "               'money': {},\n",
    "               'family': {},\n",
    "               'health': {}\n",
    "              }\n",
    "\n",
    "for year, data in enumerate(sorted_data_norm):\n",
    "    for gender in data:\n",
    "        for datum in data[gender]:\n",
    "            for topic in datum:\n",
    "                if topic[0] in topics_norm.keys():\n",
    "                    if gender in topics_norm[topic[0]].keys():\n",
    "                        topics_norm[topic[0]][gender].append(topic[1])\n",
    "                    else:\n",
    "                        topics_norm[topic[0]][gender] = [topic[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d19f9-e672-4cf4-b8cd-4bf341c710dc",
   "metadata": {},
   "source": [
    "Now we can finally get started on the plots. The first one we'll create is a dynamic graph showing the evolution of the distribution of topics per gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ba7931-d386-4943-b6bd-08716a0dfa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will plot the distribution of topics per gender\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# we prepare our data for plotting\n",
    "genders_show = [' '.join([word.capitalize() for word in gender.split()]) for gender in genders]\n",
    "\n",
    "plot_values_norm = {'gender': [],\n",
    "                    'topic': [],\n",
    "                    'percentage': [],\n",
    "                    'date':[]\n",
    "                   }\n",
    "\n",
    "for gender in genders:\n",
    "    for topic in topics_norm:\n",
    "        for index, val in enumerate(topics_norm[topic][gender]):\n",
    "            if all(np.array(topics_norm[topic][gender][index:]) == 0):\n",
    "                break\n",
    "            month = index % 12 + 1\n",
    "            year = 2015 + index // 12\n",
    "            plot_values_norm['gender'].append(gender)\n",
    "            plot_values_norm['topic'].append(topic)\n",
    "            plot_values_norm['percentage'].append(val)\n",
    "            plot_values_norm['date'].append(f'{year}-{month:02}')\n",
    "            \n",
    "plot_values_norm = pd.DataFrame.from_dict(plot_values_norm)\n",
    "\n",
    "fig = px.bar(plot_values_norm,\n",
    "             x='gender',\n",
    "             y='percentage',\n",
    "             color='topic',\n",
    "             title='Distribution of Key Topics per Gender',\n",
    "             animation_frame='date',\n",
    "             height=750,\n",
    "             range_y=[min(plot_values_norm['percentage']), max(plot_values_norm['percentage'])],\n",
    "             barmode='group',\n",
    "             hover_name='topic',\n",
    "             hover_data={'gender':False, 'topic':False, 'percentage':True, 'date':False},\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    # xaxis_title=\"Gender\",\n",
    "    yaxis_title=\"Percentage of Words about Topic in Quotes by Gender\",\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/topic_in_gender.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5d535-b0ab-4378-a8d1-89a618ab3826",
   "metadata": {},
   "source": [
    "The second plot is also a dynamic graph, but this time showing the distribution of genders inside a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e6eceea-0cef-4709-bb09-fcf03bb12b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for showing the gender distribution for any topic\n",
    "# this cell will be for the overall topic evolution\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# we prepare our data for plotting\n",
    "genders_show = [' '.join([word.capitalize() for word in gender.split()]) for gender in genders]\n",
    "\n",
    "plot_values_abs = {'gender': [],\n",
    "                   'topic': [],\n",
    "                   'counts': [],\n",
    "                   'date':[]\n",
    "                  }\n",
    "\n",
    "for gender in genders:\n",
    "    for topic in topics:\n",
    "        for index, val in enumerate(topics[topic][gender]):\n",
    "            if all(np.array(topics[topic][gender][index:]) == 0):\n",
    "                break\n",
    "            month = index % 12 + 1\n",
    "            year = 2015 + index // 12\n",
    "            plot_values_abs['gender'].append(gender)\n",
    "            plot_values_abs['topic'].append(topic)\n",
    "            plot_values_abs['counts'].append(val)\n",
    "            plot_values_abs['date'].append(f'{year}-{month:02}')\n",
    "            \n",
    "plot_values_abs = pd.DataFrame.from_dict(plot_values_abs)\n",
    "\n",
    "fig = px.bar(plot_values_abs,\n",
    "             x='topic',\n",
    "             y='counts',\n",
    "             color='gender',\n",
    "             title='Distribution of Genders per Key Topic',\n",
    "             animation_frame='date',\n",
    "             height=750,\n",
    "             range_y=[max(min(plot_values_abs['counts']),1), max(plot_values_abs['counts'])],\n",
    "             log_y = True,\n",
    "             barmode='group',\n",
    "             hover_name='gender',\n",
    "             hover_data={'gender':False, 'topic':False, 'counts':True, 'date':False},\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    #xaxis_title=\"Topic\",\n",
    "    yaxis_title=\"Word Counts by Gender per Topic\",\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/gender_in_topic.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9e9a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this part of our analysis we will be using the scores generated by the sentiment intensity analyzer of nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through years\n",
    "for year in years:\n",
    "    print(f\"Analysing year {year}...\", end=\" \")\n",
    "    \n",
    "    # data location\n",
    "    data_file = 'quotes-'+str(year)+'-prep.json.bz2'\n",
    "    data_path = data_folder + data_file\n",
    "    sent_analysis_folder = data_folder + 'sentiment_analysis/'\n",
    "    \n",
    "    # Load by chunks\n",
    "    f = bz2.open(data_path, \"rb\")\n",
    "    data=pd.read_json(f, lines=True, chunksize=10000)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    start_progress = timeit.default_timer()\n",
    "    progress_step = 100\n",
    "    \n",
    "    with bz2.open(sent_analysis_folder +str(year)+ '.json.bz2', \"w\") as fn:\n",
    "        for i_chunk, chunk in enumerate(data):\n",
    "            # Print progress\n",
    "            if i_chunk%progress_step == 0:\n",
    "                stop_progress = timeit.default_timer()\n",
    "                print(f'Time since last: {stop_progress-start_progress:.1f}s\\n')\n",
    "                print(f\"Processing chunks {i_chunk}-{i_chunk+progress_step-1}\")\n",
    "                start_progress = timeit.default_timer()\n",
    "                \n",
    "            # Process chunk\n",
    "            chunk.drop(columns=['probas','phase'], inplace=True)\n",
    "            chunk['sentiment_score']=chunk['quotation'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "            chunk['year']=chunk['date'].apply(lambda x: x.year)\n",
    "            chunk['month']=chunk['date'].apply(lambda x:x.month)\n",
    "            \n",
    "            # Write to json.bz2 file\n",
    "            write = chunk.to_json(fn, lines=True, orient='records')\n",
    "            \n",
    "    stop = timeit.default_timer()\n",
    "    print(f'Total time: {stop-start:.1f}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20306b",
   "metadata": {},
   "source": [
    "In order to observe the differences between conservative and liberal news sources, to represent these two categories we have created 2 lists that contain some of the most popular liberal and conservative news sources according to ThoughtCo and Aelieve Digital Marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define lists of representative liberal and conservative news sources\n",
    "libr=['cnn', 'huffingtonpost', 'nytimes', 'politico', 'slate', 'abcnews', 'dailykos', 'washingtonpost', \n",
    "      'time', 'theatlantic']\n",
    "cons=['nationalreview', 'spectator', 'theamericanconservative', 'washingtontimes', 'thenewamerican', 'freebeacon',\n",
    "      'frontpagemag', 'theblaze', 'humanevents', 'cnsnews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0631e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Never ever run again\n",
    "\n",
    "df_repr=pd.DataFrame([])  #to keep only the representative websites in the list above\n",
    "df_sent=pd.DataFrame([])  #to keep the aggregated values\n",
    "\n",
    "# Loop through years\n",
    "for year in years:\n",
    "    print(f\"Analysing year {year}...\", end=\" \")\n",
    "    \n",
    "    # Load by chunks\n",
    "    f = bz2.open(sent_analysis_folder +str(year)+ '.json.bz2', \"rb\")\n",
    "    data=pd.read_json(f, lines=True, chunksize=10000)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    start_progress = timeit.default_timer()\n",
    "    progress_step = 100\n",
    "\n",
    "    for i_chunk, chunk in enumerate(data):\n",
    "        # Print progress\n",
    "        if i_chunk%progress_step == 0:\n",
    "            stop_progress = timeit.default_timer()\n",
    "            print(f'Time since last: {stop_progress-start_progress:.1f}s\\n')\n",
    "            print(f\"Processing chunks {i_chunk}-{i_chunk+progress_step-1}\")\n",
    "            start_progress = timeit.default_timer()\n",
    "            \n",
    "        # Process chunk\n",
    "        # aggregate over year-month-gender and save to df_sent\n",
    "        df_sent=df_sent.append(chunk.groupby(['year','month','gender']).agg({'sentiment_score':'mean', 'numOccurrences':'sum'}))\n",
    "        # keep only the quotes appeared in the representative lists\n",
    "        chunk=chunk[chunk['websites'].apply(lambda x: 1 if any(i in x for i in libr+cons) else 0)==1]\n",
    "        # explode each website list and drop the ones that are not in the lists\n",
    "        chunk=chunk.explode('websites')\n",
    "        chunk=chunk[chunk['websites'].isin(libr+cons)].drop_duplicates(['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences', 'websites', 'gender', 'sentiment_score'])\n",
    "        df_repr=df_repr.append(chunk)\n",
    "                     \n",
    "    stop = timeit.default_timer()\n",
    "    print(f'Total time: {stop-start:.1f}s\\n')\n",
    "\n",
    "df_sent.reset_index(inplace=True)\n",
    "df_repr.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the representative data into json\n",
    "\n",
    "start = timeit.default_timer()\n",
    "start_progress = timeit.default_timer()\n",
    "progress_step = 100\n",
    "\n",
    "with bz2.open(sent_analysis_folder + 'representatives' + '.json.bz2', \"w\") as f:\n",
    "    for i in range(0, math.ceil(df_repr.shape[0]/10000)):\n",
    "        temp=df_repr[i*10000 : (i+1)*10000]\n",
    "        \n",
    "        # Print progress\n",
    "        stop_progress = timeit.default_timer()\n",
    "        print(f\"Saving rows {i*10000}-{(i+1)*10000}\")\n",
    "        print(f'Time since last: {stop_progress-start_progress:.1f}s\\n')\n",
    "        start_progress = timeit.default_timer()\n",
    "            \n",
    "        # Write to json.bz2 file\n",
    "        write = temp.to_json(f, lines=True, orient='records')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'Total time: {stop-start:.1f}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e00d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the genders subject to analysis\n",
    "keep_gender_list=['male', 'female', 'transgender male', 'transgender female', 'non-binary', 'genderfluid']\n",
    "df_sent=df_sent[df_sent['gender'].isin(keep_gender_list)].reset_index(drop=True)\n",
    "df_repr=df_repr[df_repr['gender'].isin(keep_gender_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efcc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of representative set and save \n",
    "df_repr_sum=df_repr.groupby(['year','month','gender', 'websites']).agg({'sentiment_score':'mean', 'quoteID':'count'}).reset_index()\n",
    "df_repr_sum.rename(columns={'quoteID': \"quoteID_count\"},inplace=True)\n",
    "\n",
    "with bz2.open(sent_analysis_folder + 'representatives_summary' + '.json.bz2', \"w\") as f:\n",
    "    write = df_repr_sum.to_json(f, lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2765",
   "metadata": {},
   "source": [
    "Since the aggregation of the overall summary table df_sent was done chunk by chunk, we need to correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ce142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the aggregation in summary table, keep both sentiment_score and numOccurrences in the table final table\n",
    "\n",
    "df_sent2=df_sent.groupby(['year','month','gender']).apply(lambda x: (x['sentiment_score']*x['numOccurrences']).sum()/x['numOccurrences'].sum()).reset_index()\n",
    "temp=df_sent.groupby(['year','month','gender'])['numOccurrences'].agg('sum').reset_index()\n",
    "df_sent2=df_sent2.merge(temp, how='inner', on=['year','month','gender']) \n",
    "df_sent2.rename(columns={0: \"avg_sentiment_score\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb190317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinde 'year'&'month'\n",
    "df_sent2['year']=df_sent2['year'].apply(str)\n",
    "df_sent2['month']=df_sent2['month'].apply(str)\n",
    "df_sent2['date']=df_sent2['year']+'-'+df_sent2['month']\n",
    "\n",
    "df_repr_sum['year']=df_repr_sum['year'].apply(str)\n",
    "df_repr_sum['month']=df_repr_sum['month'].apply(str)\n",
    "df_repr_sum['date']=df_repr_sum['year']+'-'+df_repr_sum['month']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d131ea",
   "metadata": {},
   "source": [
    "Let's see how the average sentiment scores have changed over the time for each gender category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c608a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_sent2, x=\"date\", y=\"avg_sentiment_score\", color='gender', \n",
    "              color_discrete_sequence=[\"#636EFA\", \"#EF553B\",  \"#00CC96\", \"#AB63FA\", \"#FFA15A\", \"#19D3F3\"],\n",
    "              category_orders={\"gender\":[\"male\", \"female\", \"transgender male\", \"transgender female\", \"non-binary\", \"genderfluid\"]}\n",
    "             )\n",
    "\n",
    "fig.update_layout(title_text='Average sentiment scores of quotes from different genders over time', title_x=0.5,\n",
    "                  xaxis_title='Date', yaxis_title='Average sentiment score', legend_title_text='Gender')\n",
    "\n",
    "fig.write_html('./plotly/sent_vs_time_allgenders_sep.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd72423",
   "metadata": {},
   "source": [
    "To be able to make more significant comparisons we combine transgender-male, transgender-female, genderfluid and non-binary genders in a single group named 'others' and plot the graph again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501636b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the others group and rearrange the summary table \n",
    "df_sent2['gender']=np.where(df_sent2['gender'].isin(['male','female']), df_sent2['gender'] ,'others')\n",
    "\n",
    "df_sent2_others=df_sent2.groupby(['year','month','gender','date']).apply(lambda x: (x['avg_sentiment_score']*x['numOccurrences']).sum()/x['numOccurrences'].sum()).reset_index()\n",
    "temp=df_sent2.groupby(['year','month','gender','date'])['numOccurrences'].agg('sum').reset_index()\n",
    "df_sent2_others=df_sent2_others.merge(temp, how='inner', on=['year','month','gender', 'date'])\n",
    "\n",
    "df_sent2_others.rename(columns={0: \"avg_sentiment_score\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_sent2_others, x=\"date\", y=\"avg_sentiment_score\", color='gender', \n",
    "              color_discrete_sequence=[\"#636EFA\", \"#EF553B\",  \"#FF97FF\"],\n",
    "              category_orders={\"gender\":[\"male\", \"female\", \"others\"]}\n",
    "             )\n",
    "\n",
    "fig.update_layout(title_text='Average sentiment scores of quotes from different genders over time', title_x=0.5,\n",
    "                  xaxis_title='Date', yaxis_title='Average sentiment score', legend_title_text='Gender')\n",
    "\n",
    "fig.write_html('./plotly/sent_vs_time_allgenders.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7344a",
   "metadata": {},
   "source": [
    "We can see that the average sentiment scores attached to male and female quotes lie between 0.15 and 0.26, which is close to neutral but still positive. On the other hand, quotes belonging to transgender-male, transgender-female, genderfluid and non-binary genders, which are aggregated in a single group named 'others', tend to have more oscillating average sentiment scores. For these genders, we can observe both positive and negative average scores that cover a wider range from -0.15 to 0.32. \n",
    "\n",
    "When we focus on quotes from males and females, we see that almost consistently scores of male quotes lie above the scores of female quotes. Whereas there is no such consistent pattern for other genders. Let's take a closer look into this consistent difference between male and female quotes.\n",
    "\n",
    "Let's see if this situation continues when we divide news sources into two categories as liberal and conservative. First, we make an overall comparison between liberal (denoted by L) and conservative (denoted by C) news sources, then take a deep dive into each website separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column specifying the class of the news source\n",
    "df_sent3=df_sent2[df_sent2['gender'].isin(['male','female'])]\n",
    "df_repr_sum2=df_repr_sum[df_repr_sum['gender'].isin(['male','female'])]\n",
    "df_repr_sum2['L/C']=df_repr_sum2['websites'].apply(lambda x: np.where(x in libr,'L','C'))\n",
    "\n",
    "#Aggregate on to the new column\n",
    "df_repr_sum3=df_repr_sum2.groupby(['year','month','gender','L/C']).apply(lambda x: (x['sentiment_score']*x['quoteID_count']).sum()/x['quoteID_count'].sum()).reset_index()\n",
    "temp=df_repr_sum2.groupby(['year','month','gender','L/C'])['quoteID_count'].agg('sum').reset_index()\n",
    "df_repr_sum3=df_repr_sum3.merge(temp, how='inner', on=['year','month','gender','L/C'])\n",
    "\n",
    "df_repr_sum3.rename(columns={0: \"avg_sentiment_score\"},inplace=True)\n",
    "\n",
    "#Combine year and month\n",
    "df_repr_sum3['year']=df_repr_sum3['year'].apply(str)\n",
    "df_repr_sum3['month']=df_repr_sum3['month'].apply(str)\n",
    "df_repr_sum3['date']=(df_repr_sum3['year']+'-'+df_repr_sum3['month'])\n",
    "df_repr_sum3['date']=pd.to_datetime(df_repr_sum3['date'], format='%Y-%m')\n",
    "df_repr_sum3.sort_values(by='date', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab818d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#Subsets for the subplots\n",
    "fm_l=df_repr_sum3[(df_repr_sum3['gender']=='female')&(df_repr_sum3['L/C']=='L')]\n",
    "fm_c=df_repr_sum3[(df_repr_sum3['gender']=='female')&(df_repr_sum3['L/C']=='C')]\n",
    "ml_l=df_repr_sum3[(df_repr_sum3['gender']=='male')&(df_repr_sum3['L/C']=='L')]\n",
    "ml_c=df_repr_sum3[(df_repr_sum3['gender']=='male')&(df_repr_sum3['L/C']=='C')]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Liberal\", \"Conservative\"))\n",
    "\n",
    "fig.append_trace(go.Scatter(x=ml_l['date'], y=ml_l['avg_sentiment_score'], name='Male-L', marker={ 'color':'#636EFA'}), \n",
    "                 row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=fm_l['date'], y=fm_l['avg_sentiment_score'], name='Female-L', marker={ 'color':'#EF553B'}),\n",
    "                 row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=ml_c['date'], y=ml_c['avg_sentiment_score'], name='Male-C', marker={ 'color':'#636EFA'}),\n",
    "                 row=2, col=1)\n",
    "fig.append_trace(go.Scatter(x=fm_c['date'], y=fm_c['avg_sentiment_score'], name='Female-C', marker={ 'color':'#EF553B'}),\n",
    "                 row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average sentiment score\")\n",
    "fig.update_layout(title_text=\"Average sentiment scores of quotes over time in liberal and conservative news sources\", title_x=0.5, \n",
    "                  legend_title_text='Gender')\n",
    "\n",
    "fig.write_html('./plotly/sent_vs_time_vs_lib&cons_male&female.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26a341",
   "metadata": {},
   "source": [
    "From the plot above, we can see that in liberal news sources, sentiment scores of male and female quotes are close to each other, but the female quotes portrayed in these news sources have slightly higher positive scores compared to males'. On the other hand, in conservative news sources, we see that the gap between the male and female quotes' sentiment scores is a little wider compared to the gap in the liberal graph. Also, we can see that contrary to the quotes in liberal news sources, in conservative news sources the coverage given to male quotes have higher positive sentiment scores than the female quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_compare=fm_l.merge(ml_l, how='inner', on=['year','month','L/C'])\n",
    "c_compare=fm_c.merge(ml_c, how='inner', on=['year','month','L/C'])\n",
    "\n",
    "l_compare['diff']=l_compare['avg_sentiment_score_x']-l_compare['avg_sentiment_score_y']\n",
    "c_compare['diff']=c_compare['avg_sentiment_score_x']-c_compare['avg_sentiment_score_y']\n",
    "\n",
    "print('Mean female-male sentiment score diff in L: ', np.abs(l_compare['diff']).mean())\n",
    "print('Mean female-male sentiment score diff in C: ', np.abs(c_compare['diff']).mean())\n",
    "\n",
    "print('\\nFemale sent avg in L: ', fm_l['avg_sentiment_score'].mean())\n",
    "print('Female sent avg in C: ', fm_c['avg_sentiment_score'].mean())\n",
    "\n",
    "print('\\nMale sent avg in L: ', ml_l['avg_sentiment_score'].mean())\n",
    "print('Male sent avg in C: ', ml_c['avg_sentiment_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Table(header=dict(values=['Mean of', 'Value']),\n",
    "                 cells=dict(values=[['Female-male quotes\\' sentiment score difference in L',\n",
    "                                     'Female-male quotes\\' sentiment score difference in C',\n",
    "                                     'Female quotes\\' sentiment score in L', \n",
    "                                     'Female quotes\\' sentiment score in C',\n",
    "                                     'Male quotes\\' sentiment score in L',\n",
    "                                     'Male quotes\\' sentiment score in C'], \n",
    "                                    [round(np.abs(l_compare['diff']).mean(),3), \n",
    "                                     round(np.abs(c_compare['diff']).mean(),3), \n",
    "                                     round(fm_l['avg_sentiment_score'].mean(),3), \n",
    "                                     round(fm_c['avg_sentiment_score'].mean(),3), \n",
    "                                     round(ml_l['avg_sentiment_score'].mean(),3),\n",
    "                                     round(ml_c['avg_sentiment_score'].mean(),3)]\n",
    "                                   ]\n",
    "                           )\n",
    "                              )\n",
    "                     ]\n",
    "               )\n",
    "fig.write_html(\"./plotly/mean_table.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e2e77",
   "metadata": {},
   "source": [
    "This table shows that the mean difference between the average sentiment scores of female and male quotes in conservative news sources is twice of the liberal news sources. Also, we see that there is a 14% difference in the mean sentiment scores of female quotes in liberal and conservative news sources and a 38% difference for the male quotes. These results and the graphs above brings up two questions:\n",
    "\n",
    "**'Do the conservative news sources tend to give more coverage to quotes with higher positivity from males?'**\n",
    "\n",
    "**'Do the liberal news sources tend to give more coverage to quotes with higher positivity from females?'**\n",
    "\n",
    "Let's have a look at the distributions of the sentiment scores of quotes from males and females in liberal and conservative news sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We go back to the summary table df_repr and divide it to 4 new tables:\n",
    "#female&liberal, female&conservative, male&liberal, male&conservative\n",
    "df_repr=df_repr[df_repr['gender'].isin(['female','male'])].reset_index(drop=True)\n",
    "df_repr['L/C']=df_repr['websites'].apply(lambda x: np.where(x in libr,'L','C'))\n",
    "\n",
    "df_repr_fm_l=df_repr[(df_repr['gender']=='female')&(df_repr['L/C']=='L')].reset_index()\n",
    "df_repr_fm_c=df_repr[(df_repr['gender']=='female')&(df_repr['L/C']=='C')].reset_index()\n",
    "\n",
    "df_repr_ml_l=df_repr[(df_repr['gender']=='male')&(df_repr['L/C']=='L')].reset_index()\n",
    "df_repr_ml_c=df_repr[(df_repr['gender']=='male')&(df_repr['L/C']=='C')].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Box(y=df_repr_ml_l['sentiment_score'], name='Male-Liberal', marker_color=\"#636EFA\"))\n",
    "fig.add_trace(go.Box(y=df_repr_ml_c['sentiment_score'], name='Male-Conservative', marker_color=\"#636EFA\"))\n",
    "fig.add_trace(go.Box(y=df_repr_fm_l['sentiment_score'], name='Female-Liberal', marker_color=\"#EF553B\"))\n",
    "fig.add_trace(go.Box(y=df_repr_fm_c['sentiment_score'], name='Female-Conservative', marker_color=\"#EF553B\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title_text='Sentiment score distribution in liberal and conservative news sources ', title_x=0.5,\n",
    "                  xaxis_title='Gender & News Source', yaxis_title='Sentiment score', \n",
    "                  legend_title_text='Genders')\n",
    "fig.write_html(\"./plotly/box_sent_lib&cons_male&female.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ec6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Var of male quotes in liberal news sources: ', df_repr_ml_l['sentiment_score'].var())\n",
    "print('Var of male quotes in conservative news sources: ', df_repr_ml_c['sentiment_score'].var())\n",
    "\n",
    "print('\\nVar of female quotes in liberal news sources: ', df_repr_fm_l['sentiment_score'].var())\n",
    "print('Var of female quotes in conservative news sources: ', df_repr_fm_c['sentiment_score'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23ef15",
   "metadata": {},
   "source": [
    "From the boxplot we can directly see that there is a general shift towards more positive sentiment scores of quotes belonging to men in conservative news sources. On the other side, we see that although the median and third quartile values of sentiment scores of female quotes are very similar in both liberal and conservative news sources, the first quartile value, which falls in the area of negative sentiment, is higher in the liberal news sources.\n",
    "\n",
    "Since the variance of the distributions are similar, to see if the differences we have seen in the sentiment scores with respect to news source category L/C in the sample we have created with the chosen news sources are significant for the whole population, we perform a one-sided independent student t-test with the following null hypotheses:\n",
    "\n",
    "    - Test 1 - H0: The mean sentiment score of quotes from males in conservative news sources is less than that of liberal \n",
    "    news sources.\n",
    "    - Test 2 - H0: The mean sentiment score of quotes from females in liberal news sources is less than that of conservative\n",
    "    news sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90192d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Student's t-test\n",
    "statistic, pvalue = stats.ttest_ind(df_repr_ml_c['sentiment_score'], df_repr_ml_l['sentiment_score'], alternative='greater')\n",
    "print(\"Student's t-test p-value for 'sentiment_score' of males' quotes in L and C: \" + str(pvalue/2))\n",
    "\n",
    "statistic2, pvalue2 = stats.ttest_ind(df_repr_fm_l['sentiment_score'], df_repr_fm_c['sentiment_score'], alternative='greater')\n",
    "print(\"Student's t-test p-value for 'sentiment_score' of females' quotes in L and C: \" + str(pvalue2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b33bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Table(header=dict(values=['Test number', 'p-value/2']),\n",
    "                 cells=dict(values=[['Test - 1',\n",
    "                                     'Test - 2'], \n",
    "                                    [round(pvalue/2,5), \n",
    "                                     round(pvalue2/2,5)]\n",
    "                                   ]\n",
    "                           )\n",
    "                              )\n",
    "                     ]\n",
    "               )\n",
    "fig.write_html(\"./plotly/p_table.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16c44d",
   "metadata": {},
   "source": [
    "Since for both tests, the p-values are less than the significance level threshold of 0.005, we have enough evidence to reject the null hypotheses and conclude that:\n",
    "\n",
    "    - The mean sentiment score of quotes from males portrayed in conservative news sources is higher than that of liberal \n",
    "      news sources. In other words, conservative news sources tend to give more coverage to quotes with higher positivity \n",
    "      from males compared to liberals\n",
    "    - The mean sentiment score of quotes from females portrayed in liberal news sources is greater than that of conservative \n",
    "      news sources. In other words, liberal news sources tend to give more coverage to quotes with higher positivity from \n",
    "      females compared to conservatives.\n",
    "\n",
    "Now that we had an overview of the sentiment score differences with respect to gender and news source category, let's have a closer look into the behavior of individual news sources within our representative news source lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eaad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_sum=df_repr.groupby(['websites','gender', 'L/C']).agg({'sentiment_score':'mean', 'quoteID':'count'}).reset_index()\n",
    "\n",
    "website_sum.rename(columns={'quoteID': \"quoteID_count\"},inplace=True)\n",
    "website_sum.rename(columns={'sentiment_score': \"avg_sentiment_score\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77154f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(website_sum[website_sum['L/C']=='L'], x = 'websites', y = 'avg_sentiment_score', \n",
    "                 size = 'quoteID_count', color = 'gender', size_max=50, \n",
    "                 color_discrete_map={\"male\": \"#636EFA\", \"female\": \"#EF553B\"},\n",
    "                 category_orders={\"gender\":[\"male\", \"female\"]})\n",
    "\n",
    "fig.update_layout(title = \"Sentiment scores in liberal news sources\", title_x=0.5,\n",
    "                  xaxis_title='News source websites', yaxis_title='Average sentiment score', legend_title_text='Gender')\n",
    "\n",
    "fig.write_html(\"./plotly/websites_L.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cdb77",
   "metadata": {},
   "source": [
    "We can see that among the selected liberal news sources The New York Times is the one that portrays quotes with the most similar sentiment scores from males and females, followed by Slate, The Atlantic, Politico and CNN. Whereas Huffington Post is the one with the biggest difference in sentiment scores, followed by Time Magazine. We can also note that 9 out of 10 of the representative liberal news sources have quotes with higher sentiment scores from females, only 1 has a reversed trend, which is The Washington Post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84690fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(website_sum[website_sum['L/C']=='C'], x = 'websites', y = 'avg_sentiment_score', \n",
    "                 size = 'quoteID_count', color = 'gender',size_max=50, \n",
    "                 color_discrete_map={\"male\": \"#636EFA\", \"female\": \"#EF553B\"},\n",
    "                 category_orders={\"gender\":[\"male\", \"female\"]})\n",
    "\n",
    "fig.update_layout(title = \"Sentiment scores in conservative news sources\", title_x=0.5,\n",
    "                  xaxis_title='News source websites', yaxis_title='Average sentiment score', legend_title_text='Gender')\n",
    "\n",
    "fig.write_html(\"./plotly/websites_C.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a43dd",
   "metadata": {},
   "source": [
    "We can see that among the selected conservative news sources National Review is the one that portrays quotes with the most similar sentiment scores from males and females, followed by The American Conservative, The American Spectator and The Blaze. Whereas Human Events and The Washington Times are the ones having the biggest difference in sentiment scores. The Washington Times also constitutes the majority of the quotes in the conservative group and therefore is an important factor in the trend towards more positive quotes from men in conservative news sources. By looking at the sizes of the data points in the graphs, in addition to the sentimental differences, we can see that conservative news sources make less use of quotations than liberal news sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3ff43-63bd-4c4a-8c98-3df6f4a13318",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Text Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e7517-179e-496a-95aa-8684328f691e",
   "metadata": {},
   "source": [
    "To measure the text complexity, the process is very similar all the other parts. We'll start by analyzing the data and extracting the relevant information, in this case the complexity information, per gender, per date, per website.\n",
    "\n",
    "These files are going to be saved in the `./data_processed` folder, under the names `complexity_<year>`. Furthermore, so we can do a more significant statistical analysis, we'll also save the standard deviation for all of these, in the files called `std_complexity_<year>`.\n",
    "\n",
    "The internal organization of these files is as follows. They separate the data by website, gender and date. So in each file we have one dictionary where the keys are the websites. In each entry we have another dictionary, where the keys are the genders. Finally, for each entry we have a list with 12 elements, one for each month, where we'll save the text complexity / standard deviation for that month. In short, if `root` is the root dictionary extracted from the folder, then to go all the way into it we need `root[website][gender][month]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df339ca-2ded-4e6d-8f55-89a12217e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the means\n",
    "from src.text_scores import *\n",
    "\n",
    "# define the liberal and conservative newspapers\n",
    "libr=['cnn', 'huffingtonpost', 'huffpost', 'nytimes', 'politico', 'slate', 'abcnews', 'dailykos', 'washingtonpost', 'time', 'theatlantic']\n",
    "cons=['nationalreview', 'spectator', 'theamericanconservative', 'washingtontimes', 'thenewamerican', 'freebeacon', 'frontpagemag', 'theblaze', 'humanevents', 'cnsnews']\n",
    "websites = libr + cons\n",
    "\n",
    "# and define the key genders to consider\n",
    "genders = ['male', 'female', 'transgender male', 'transgender female', 'non-binary', 'genderfluid']\n",
    "\n",
    "dfs_quotes = []\n",
    "# extract all the data taken from quotebank\n",
    "for file in glob.glob(data_folder + data_file):\n",
    "    dfs_quotes.append(pd.read_json(file, lines=True, chunksize=1e4))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# go through each file (aka year) and process them\n",
    "for year, file in enumerate(dfs_quotes):\n",
    "    complexity = {}\n",
    "    i = 0\n",
    "    # we need to process them by chunks, they are too big\n",
    "    for chunk in file:\n",
    "        # create the temporary variables to store the information\n",
    "        tokens = {}\n",
    "        quotes = chunk['tokens'].tolist()\n",
    "        date = chunk['date'].tolist()\n",
    "        gender = chunk['gender'].tolist()\n",
    "        sources = chunk['websites'].tolist()\n",
    "        \n",
    "        # now we'll iterate through all the quotes\n",
    "        for index, quote in enumerate(quotes):\n",
    "            # if it's one of the genders we're not looking into, then we skip\n",
    "            if gender[index] not in genders:\n",
    "                continue\n",
    "            # and now we iterate through all the news sources\n",
    "            for source in sources[index]:\n",
    "                # if it's not one of the sources we're considering, we skip it\n",
    "                if source not in websites:\n",
    "                    continue\n",
    "                # if the source is not yet in the dictionary, we add it\n",
    "                if source not in tokens.keys():\n",
    "                    tokens[source] = {}\n",
    "                # and if the gender is not yet in tokens[source] we also add it\n",
    "                if gender[index] not in tokens[source].keys():\n",
    "                    tokens[source][gender[index]] = [[] for _ in range(12)]\n",
    "                # after all of this just add the quote on the right place (source -> gender -> month)\n",
    "                tokens[source][gender[index]][int(str(date[index])[5:7]) - 1].append(quote)\n",
    "        \n",
    "        # now that we have sorted the quotes we can process them\n",
    "        # iterate through all the keys\n",
    "        for website in tokens.keys():\n",
    "            # if this is the first time seeing this website, add it to the complexity dict\n",
    "            if website not in complexity.keys():\n",
    "                complexity[website] = {}\n",
    "            # iterate through all keys in tokens[website]\n",
    "            for gender in tokens[website].keys():\n",
    "                # if it's the first time seeing this gender in this website, add it\n",
    "                if gender not in complexity[website].keys():\n",
    "                    complexity[website][gender]= [[0,0] for _ in range(12)]\n",
    "                # finally, go through all the months and add them\n",
    "                for month, quotes in enumerate(tokens[website][gender]):\n",
    "                    for quote in quotes:\n",
    "                        quote_join = ' '.join(quote)\n",
    "                        # if it's a non-empty quote calculate it's complexity and add one more counter for the mean\n",
    "                        if len(quote_join) != 0:\n",
    "                            complexity[website][gender][month][0] += dale_chall_score(quote_join)\n",
    "                            complexity[website][gender][month][1] += 1\n",
    "\n",
    "        # periodically save the data so we can resume if needed\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            with open(f'complexity_{year + 2015}.txt', 'w') as f:\n",
    "                f.write(f'Chunks processed: {i}\\n')\n",
    "                f.write(json.dumps(complexity))\n",
    "            print(i, end = ',')\n",
    "\n",
    "            \n",
    "    # now we decompose the means\n",
    "    # go through all websites, genders and dates\n",
    "    for website in complexity.keys():\n",
    "        for gender in complexity[website].keys():\n",
    "            for month, scores in enumerate(complexity[website][gender]):\n",
    "                # take the mean of the complexity scores\n",
    "                if scores[1] != 0:\n",
    "                    complexity[website][gender][month] = scores[0] / scores[1]\n",
    "                else:\n",
    "                    complexity[website][gender][month] = 0\n",
    "    # and save it all to a file\n",
    "    with open(f'complexity_{year + 2015}.txt', 'w') as f:\n",
    "        f.write(json.dumps(complexity))\n",
    "    print()\n",
    "\n",
    "print(f'Time to analyze all chunks {timeit.default_timer() - start}!!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c7b62-cd1e-4d35-bd58-49b7e26dc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviations\n",
    "from src.text_scores import *\n",
    "\n",
    "# go through all the files taken from quotebank\n",
    "dfs_quotes = []\n",
    "for file in glob.glob(data_folder + data_file):\n",
    "    dfs_quotes.append(pd.read_json(file, lines=True, chunksize=1e4))\n",
    "\n",
    "# extract the means we just calculated\n",
    "# we need the means for the standard deviation\n",
    "complexity_means = []\n",
    "for file in glob.glob('./complexity*.txt'):\n",
    "    with open(file, 'r') as f:\n",
    "        complexity_means.append(eval(f.read()))\n",
    "\n",
    "        \n",
    "start = timeit.default_timer()\n",
    "# for in depth comments of the process see the cell above, it is very similar\n",
    "for year, file in enumerate(dfs_quotes):\n",
    "    complexity_std = {}\n",
    "    i = 0\n",
    "    for chunk in file:\n",
    "        tokens = {}\n",
    "        quotes = chunk['tokens'].tolist()\n",
    "        date = chunk['date'].tolist()\n",
    "        gender = chunk['gender'].tolist()\n",
    "        sources = chunk['websites'].tolist()\n",
    "        \n",
    "        for index, quote in enumerate(quotes):\n",
    "            if gender[index] not in genders:\n",
    "                continue\n",
    "            for source in sources[index]:\n",
    "                if source not in websites:\n",
    "                    continue\n",
    "                if source not in tokens.keys():\n",
    "                    tokens[source] = {}\n",
    "                if gender[index] not in tokens[source].keys():\n",
    "                    tokens[source][gender[index]] = [[] for _ in range(12)]\n",
    "                tokens[source][gender[index]][int(str(date[index])[5:7]) - 1].append(quote)\n",
    "        \n",
    "        for website in tokens.keys():\n",
    "            if website not in complexity_std.keys():\n",
    "                complexity_std[website] = {}\n",
    "            for gender in tokens[website].keys():\n",
    "                if gender not in complexity_std[website].keys():\n",
    "                    complexity_std[website][gender]= [[0,0] for _ in range(12)]\n",
    "                for month, quotes in enumerate(tokens[website][gender]):\n",
    "                    # print('sgmi', website, gender, month, i)\n",
    "                    for quote in quotes:\n",
    "                        quote_join = ' '.join(quote)\n",
    "                        if len(quote_join) != 0:\n",
    "                            # the only difference with this cell and the previous is how the metric is calculated\n",
    "                            # here we do (val - mean(val))**2\n",
    "                            complexity_std[website][gender][month][0] += (dale_chall_score(quote_join) - complexity_means[year][website][gender][month])**2\n",
    "                            complexity_std[website][gender][month][1] += 1\n",
    "\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            with open(f'std_complexity_{year + 2015}.txt', 'w') as f:\n",
    "                f.write(f'Chunks processed: {i}\\n')\n",
    "                f.write(json.dumps(complexity_std))\n",
    "            print(i, end = ',')\n",
    "\n",
    "            \n",
    "    for website in complexity_std.keys():\n",
    "        for gender in complexity_std[website].keys():\n",
    "            for month, scores in enumerate(complexity_std[website][gender]):\n",
    "                if scores[1] == 0 or scores[1] == 1:\n",
    "                    complexity_std[website][gender][month] = 0\n",
    "                # here we are doing the standard deviation of the mean!! so it is sum(x - mean(x)) ** 2 / (n (n - 1))\n",
    "                else:\n",
    "                    complexity_std[website][gender][month] = np.sqrt(scores[0] / (scores[1] * (scores[1] - 1)))\n",
    "                    \n",
    "    with open(f'std_complexity_{year + 2015}.txt', 'w') as f:\n",
    "        f.write(json.dumps(complexity_std))\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f'Time to analyze all chunks {timeit.default_timer() - start}!!')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391e995-9184-4280-9103-7fd145e641b6",
   "metadata": {},
   "source": [
    "After all the data has already been processed, we just need to access it in the save files, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440f59f7-91ac-44c0-adc4-de6d0e240cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_data_files = glob.glob('./data_processed/complexity*.txt')\n",
    "complexity_std_files  = glob.glob('./data_processed/std_*.txt')\n",
    "\n",
    "complexity_data = []\n",
    "complexity_std  = []\n",
    "\n",
    "# read all the fully processed files for the means\n",
    "for file in complexity_data_files:\n",
    "    with open(file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        complexity_data.append(eval(txt))\n",
    "        \n",
    "# and the same thing for the standard deviations\n",
    "for file in complexity_std_files:\n",
    "    with open(file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        complexity_std.append(eval(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3e7dd-fac8-4fa1-9e2d-5cd8e029c062",
   "metadata": {},
   "source": [
    "Next we save it into a DataFrame, because `plotly` handles information much more easily if it comes in a DataFrame. Besides, we're much more interested in the differences of complexity between men and women, so that's what we'll keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bddd7f8f-4a74-4c0c-90d8-52e5a2df85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  gender          website  complexity       std\n",
      "0     01-2015    male  washingtontimes    8.864843  0.045395\n",
      "1     02-2015    male  washingtontimes    9.039202  0.045313\n",
      "2     03-2015    male  washingtontimes    8.872466  0.042549\n",
      "3     04-2015    male  washingtontimes    8.469686  0.046238\n",
      "4     05-2015    male  washingtontimes    8.213745  0.048731\n",
      "...       ...     ...              ...         ...       ...\n",
      "3307  01-2020  female      humanevents    7.083333  3.480720\n",
      "3308  03-2020  female      humanevents   10.820000  0.000000\n",
      "3309  04-2020  female      humanevents   14.180000  0.280000\n",
      "3310  01-2020    male   huffingtonpost    9.516250  0.380283\n",
      "3311  01-2020  female   huffingtonpost    9.873750  0.730029\n",
      "\n",
      "[3312 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>website</th>\n",
       "      <th>diff_comp</th>\n",
       "      <th>diff_std</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-2015</td>\n",
       "      <td>washingtontimes</td>\n",
       "      <td>0.036797</td>\n",
       "      <td>0.127666</td>\n",
       "      <td>8.864843</td>\n",
       "      <td>8.828047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-2015</td>\n",
       "      <td>cnn</td>\n",
       "      <td>0.350108</td>\n",
       "      <td>0.135776</td>\n",
       "      <td>9.820573</td>\n",
       "      <td>9.470464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-2015</td>\n",
       "      <td>nationalreview</td>\n",
       "      <td>0.384321</td>\n",
       "      <td>0.346894</td>\n",
       "      <td>10.334224</td>\n",
       "      <td>9.949903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-2015</td>\n",
       "      <td>washingtonpost</td>\n",
       "      <td>-0.320070</td>\n",
       "      <td>0.081463</td>\n",
       "      <td>9.399163</td>\n",
       "      <td>9.719234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-2015</td>\n",
       "      <td>huffingtonpost</td>\n",
       "      <td>1.007903</td>\n",
       "      <td>0.077461</td>\n",
       "      <td>10.127426</td>\n",
       "      <td>9.119523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>04-2020</td>\n",
       "      <td>thenewamerican</td>\n",
       "      <td>0.476844</td>\n",
       "      <td>0.616195</td>\n",
       "      <td>11.632844</td>\n",
       "      <td>11.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>04-2020</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>0.315661</td>\n",
       "      <td>0.180248</td>\n",
       "      <td>9.365574</td>\n",
       "      <td>9.049912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>04-2020</td>\n",
       "      <td>spectator</td>\n",
       "      <td>3.344429</td>\n",
       "      <td>1.742403</td>\n",
       "      <td>11.236429</td>\n",
       "      <td>7.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>04-2020</td>\n",
       "      <td>slate</td>\n",
       "      <td>-1.901383</td>\n",
       "      <td>1.137778</td>\n",
       "      <td>9.000435</td>\n",
       "      <td>10.901818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>04-2020</td>\n",
       "      <td>humanevents</td>\n",
       "      <td>-0.145000</td>\n",
       "      <td>1.285856</td>\n",
       "      <td>14.035000</td>\n",
       "      <td>14.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          website  diff_comp  diff_std       male     female\n",
       "0     01-2015  washingtontimes   0.036797  0.127666   8.864843   8.828047\n",
       "1     01-2015              cnn   0.350108  0.135776   9.820573   9.470464\n",
       "2     01-2015   nationalreview   0.384321  0.346894  10.334224   9.949903\n",
       "3     01-2015   washingtonpost  -0.320070  0.081463   9.399163   9.719234\n",
       "4     01-2015   huffingtonpost   1.007903  0.077461  10.127426   9.119523\n",
       "...       ...              ...        ...       ...        ...        ...\n",
       "1092  04-2020   thenewamerican   0.476844  0.616195  11.632844  11.156000\n",
       "1093  04-2020          nytimes   0.315661  0.180248   9.365574   9.049912\n",
       "1094  04-2020        spectator   3.344429  1.742403  11.236429   7.892000\n",
       "1095  04-2020            slate  -1.901383  1.137778   9.000435  10.901818\n",
       "1096  04-2020      humanevents  -0.145000  1.285856  14.035000  14.180000\n",
       "\n",
       "[1097 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's sort all of this into a dataframe\n",
    "clean_data = {'date': [],\n",
    "              'gender': [],\n",
    "              'website': [],\n",
    "              'complexity': [],\n",
    "              'std': []\n",
    "             }\n",
    "\n",
    "# iterate through all years, websites, genders and months\n",
    "for year, data in enumerate(complexity_data):\n",
    "    for website in data.keys():\n",
    "        for gender in data[website].keys():\n",
    "            for month, complexity in enumerate(data[website][gender]):\n",
    "                # only keep complexity scores bigger than 1. Smaller than one are either results of no quotes or of very small quotes with no significance\n",
    "                if complexity > 1:\n",
    "                    clean_data['date'].append(f\"{month + 1:02}-{year + 2015}\")\n",
    "                    clean_data['gender'].append(gender)\n",
    "                    clean_data['website'].append(website)\n",
    "                    clean_data['complexity'].append(float(complexity))\n",
    "                    clean_data['std'].append(complexity_std[year][website][gender][month])\n",
    "\n",
    "# convert it to a dataframe\n",
    "df = pd.DataFrame.from_dict(clean_data)\n",
    "print(df)\n",
    "\n",
    "# create a new dataframe to fill with the difference comp_men - comp_women\n",
    "df_diff = {'date': [],\n",
    "           'website': [],\n",
    "           'diff_comp': [],\n",
    "           'diff_std': [],\n",
    "           'male': [],\n",
    "           'female': []\n",
    "          }\n",
    "\n",
    "# go through all the unique dates and websites\n",
    "for date in df['date'].unique():\n",
    "    for website in df['website'].unique():\n",
    "        # some dates don't have all websites, which gives a KeyError, this try:...except:... just gets us around that\n",
    "        try:\n",
    "            #find the male score and female score for that website and that month \n",
    "            male = df[(df['date'] == date) & (df['website'] == website) & (df['gender'] == 'male')]\n",
    "            female = df[(df['date'] == date) & (df['website'] == website) & (df['gender'] == 'female')]\n",
    "            # take the difference and calculate the std of the difference\n",
    "            diff_data = male['complexity'].values[0] - female['complexity'].values[0]\n",
    "            diff_std  = np.sqrt(male['std'].values[0]**2 + female['std'].values[0]**2)\n",
    "            \n",
    "            # save all the info into the new dict\n",
    "            df_diff['date'].append(date)\n",
    "            df_diff['website'].append(website)\n",
    "            df_diff['diff_comp'].append(diff_data)\n",
    "            df_diff['diff_std'].append(diff_std)\n",
    "            df_diff['male'].append(male['complexity'].values[0])\n",
    "            df_diff['female'].append(female['complexity'].values[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# create the new dataframe\n",
    "df_diff = pd.DataFrame.from_dict(df_diff)\n",
    "display(df_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380c035-635e-45bf-b044-5a1c87488b3c",
   "metadata": {},
   "source": [
    "Since we're not particularly interested in a temporal analysis of the complexity, we'll cluster all the temporal data, keeping only the division into websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6bda860-1ccb-49ff-a519-4c2ac3a7a668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>diff_comp</th>\n",
       "      <th>diff_std</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abcnews</td>\n",
       "      <td>0.915310</td>\n",
       "      <td>0.111445</td>\n",
       "      <td>9.372623</td>\n",
       "      <td>8.457313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>0.336825</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>9.760753</td>\n",
       "      <td>9.423928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnsnews</td>\n",
       "      <td>0.034390</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>10.217248</td>\n",
       "      <td>10.182858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dailykos</td>\n",
       "      <td>0.151414</td>\n",
       "      <td>0.204322</td>\n",
       "      <td>9.690291</td>\n",
       "      <td>9.538877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>freebeacon</td>\n",
       "      <td>0.689646</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>10.585988</td>\n",
       "      <td>9.896342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frontpagemag</td>\n",
       "      <td>0.356196</td>\n",
       "      <td>0.076858</td>\n",
       "      <td>11.158972</td>\n",
       "      <td>10.802776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>huffingtonpost</td>\n",
       "      <td>0.439255</td>\n",
       "      <td>0.022823</td>\n",
       "      <td>9.660513</td>\n",
       "      <td>9.221258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humanevents</td>\n",
       "      <td>-0.451973</td>\n",
       "      <td>0.257302</td>\n",
       "      <td>11.022474</td>\n",
       "      <td>11.474447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nationalreview</td>\n",
       "      <td>0.272510</td>\n",
       "      <td>0.069607</td>\n",
       "      <td>10.515112</td>\n",
       "      <td>10.242602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>0.257942</td>\n",
       "      <td>0.036988</td>\n",
       "      <td>9.339942</td>\n",
       "      <td>9.082001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>politico</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.032499</td>\n",
       "      <td>10.420868</td>\n",
       "      <td>10.398680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>slate</td>\n",
       "      <td>-0.069087</td>\n",
       "      <td>0.097614</td>\n",
       "      <td>10.018493</td>\n",
       "      <td>10.087580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spectator</td>\n",
       "      <td>0.258020</td>\n",
       "      <td>0.121548</td>\n",
       "      <td>10.608196</td>\n",
       "      <td>10.350176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>theamericanconservative</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>0.134907</td>\n",
       "      <td>10.791008</td>\n",
       "      <td>10.827385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>theblaze</td>\n",
       "      <td>0.350517</td>\n",
       "      <td>0.043582</td>\n",
       "      <td>9.692104</td>\n",
       "      <td>9.341586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>thenewamerican</td>\n",
       "      <td>0.198819</td>\n",
       "      <td>0.073756</td>\n",
       "      <td>11.183246</td>\n",
       "      <td>10.984427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>time</td>\n",
       "      <td>0.417455</td>\n",
       "      <td>0.087645</td>\n",
       "      <td>9.590398</td>\n",
       "      <td>9.172943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>washingtonpost</td>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.047176</td>\n",
       "      <td>9.128769</td>\n",
       "      <td>9.107907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>washingtontimes</td>\n",
       "      <td>-0.482186</td>\n",
       "      <td>0.021548</td>\n",
       "      <td>8.808466</td>\n",
       "      <td>9.290652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    website  diff_comp  diff_std       male     female\n",
       "0                   abcnews   0.915310  0.111445   9.372623   8.457313\n",
       "1                       cnn   0.336825  0.023089   9.760753   9.423928\n",
       "2                   cnsnews   0.034390  0.072825  10.217248  10.182858\n",
       "3                  dailykos   0.151414  0.204322   9.690291   9.538877\n",
       "4                freebeacon   0.689646  0.046953  10.585988   9.896342\n",
       "5              frontpagemag   0.356196  0.076858  11.158972  10.802776\n",
       "6            huffingtonpost   0.439255  0.022823   9.660513   9.221258\n",
       "7               humanevents  -0.451973  0.257302  11.022474  11.474447\n",
       "8            nationalreview   0.272510  0.069607  10.515112  10.242602\n",
       "9                   nytimes   0.257942  0.036988   9.339942   9.082001\n",
       "10                 politico   0.022188  0.032499  10.420868  10.398680\n",
       "11                    slate  -0.069087  0.097614  10.018493  10.087580\n",
       "12                spectator   0.258020  0.121548  10.608196  10.350176\n",
       "13  theamericanconservative  -0.036377  0.134907  10.791008  10.827385\n",
       "14                 theblaze   0.350517  0.043582   9.692104   9.341586\n",
       "15           thenewamerican   0.198819  0.073756  11.183246  10.984427\n",
       "16                     time   0.417455  0.087645   9.590398   9.172943\n",
       "17           washingtonpost   0.020863  0.047176   9.128769   9.107907\n",
       "18          washingtontimes  -0.482186  0.021548   8.808466   9.290652"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an auxiliary column 'count'\n",
    "df_diff['count'] = df_diff['website']\n",
    "\n",
    "# and group all the results according to index\n",
    "df_grouped = df_diff.groupby(['website'], as_index=False).agg({\n",
    "    'diff_comp' : np.mean,\n",
    "    'diff_std': np.mean,\n",
    "    'count': np.size,\n",
    "    'male': np.mean,\n",
    "    'female': np.mean\n",
    "})\n",
    "\n",
    "# the standard deviation of the mean is the mean of the standard deviations of each element divided by sqrt(n)\n",
    "df_grouped['diff_std'] /= np.sqrt(df_grouped['count'])\n",
    "df_grouped = df_grouped.drop(['count'], axis=1)\n",
    "\n",
    "display(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7459a-ce6e-4931-b54f-6ef0e6c28cba",
   "metadata": {},
   "source": [
    "Finally, we divide into liberal and conservative newspapers and plot the two graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea945b47-ecbe-4a69-a3e8-1982fb425ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "libr=['cnn', 'huffingtonpost', 'huffpost', 'nytimes', 'politico', 'slate', 'abcnews', 'dailykos', 'washingtonpost', 'time', 'theatlantic']\n",
    "cons=['nationalreview', 'spectator', 'theamericanconservative', 'washingtontimes', 'thenewamerican', 'freebeacon', 'frontpagemag', 'theblaze', 'humanevents', 'cnsnews']\n",
    "\n",
    "# split the dataframe into liberal and conservative websites\n",
    "df_libr = df_grouped[df_grouped['website'].isin(libr)]\n",
    "df_cons = df_grouped[df_grouped['website'].isin(cons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b8e5c3-13e5-4323-a1c0-f2cbaa3e17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.bar(df_libr,\n",
    "             x='website',\n",
    "             y='diff_comp',\n",
    "             title='Difference Of Text Complexity (Men - Women) on Liberal Websites',\n",
    "             width=750,\n",
    "             error_y = 'diff_std',\n",
    "             hover_name = 'website',\n",
    "             hover_data={'male':True, 'female': True, 'website':False,'diff_comp':True}\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Website\",\n",
    "    yaxis_title=\"Complexity Men - Complexity Women\",\n",
    "    xaxis={'categoryorder':'total descending'}\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/text_complexity_libr.html\")\n",
    "\n",
    "fig = px.bar(df_cons,\n",
    "             x='website',\n",
    "             y='diff_comp',\n",
    "             title='Difference Of Text Complexity (Men - Women) on Conservative Websites',\n",
    "             width=750,\n",
    "             error_y = 'diff_std',\n",
    "             hover_name = 'website',\n",
    "             hover_data={'male':True, 'female': True, 'website':False,'diff_comp':True}\n",
    "             )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Website\",\n",
    "    yaxis_title=\"Complexity Men - Complexity Women\",\n",
    "    xaxis={'categoryorder':'total descending'}\n",
    ")\n",
    "\n",
    "fig.write_html(\"./plotly/text_complexity_cons.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fd637-514d-4cab-8bd1-a58b9fb5e0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
